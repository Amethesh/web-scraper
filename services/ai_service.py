import asyncio
import json
from typing import AsyncIterable
from langchain_openai import ChatOpenAI
from langchain.callbacks import AsyncIteratorCallbackHandler
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from prompts.summarizer_prompt import SUMMARIZER_PROMPT, CHAT_PROMPT, QUERY_COMPRESS_PROMPT
from config.settings import settings

class AIChatService:
    """
    A service class for handling AI chat responses.
    """

    def __init__(self):
        self.api_key = settings.OPENAI_API_KEY

    async def ai_chat_response(self, question: str, info: str) -> AsyncIterable[str]:
        """
        Asynchronous generator for streaming AI responses using an async callback handler.
        """
        asyncCallback = AsyncIteratorCallbackHandler()

        # Initialize ChatGPT model with callbacks
        summary_llm = ChatOpenAI(
            model=settings.OPENAI_CHAT_MODEL,
            callbacks=[asyncCallback],
            temperature=0.7,
            streaming=True,
            openai_api_key=self.api_key
        )

        # Create a prompt
        prompt = PromptTemplate(template=CHAT_PROMPT, input_variables=["question", "info"])

        # Create a LangChain LLMChain instance
        summarizing_chain = prompt | summary_llm | StrOutputParser()

        # Trigger the AI response
        response_task = asyncio.create_task(
            summarizing_chain.ainvoke({"question": question, "info": info})
        )

        streamed_chunks = ""

        try:
            # Stream the tokens generated by the model
            async for token in asyncCallback.aiter():
                streamed_chunks += token
                yield json.dumps({
                    "response": token,
                    "status": "in-progress",
                    "type": "chat"
                })
        except Exception as e:
            yield json.dumps({
                "type": "chat",
                "status": "error",
                "response": "An error occurred while generating the summary.",
                "error": str(e),
            })
        finally:
            # Mark the callback as done
            asyncCallback.done.set()

        # Wait for the response task to complete
        await response_task

        # Return the finished response
        yield json.dumps({
            "response": streamed_chunks,
            "status": "finished",
            "type": "chat"
        })

    async def get_relevant_link_summary(self, question: str, urls: list) -> dict:
        """
        Generate AI response for the summary as a single string.
        """
        # Initialize ChatGPT model
        summary_llm = ChatOpenAI(
            model=settings.OPENAI_LINK_MODEL,
            temperature=0.7,
            streaming=False,
            openai_api_key=self.api_key
        )

        # Create a prompt
        prompt = PromptTemplate(template=SUMMARIZER_PROMPT, input_variables=["question", "urls"])

        # Create a LangChain LLMChain instance
        summarizing_chain = prompt | summary_llm | StrOutputParser()

        try:
            # Trigger the AI response
            response = await summarizing_chain.ainvoke({"question": question, "urls": "\n".join(urls)})
            return json.dumps({
                "type": "agent",
                "status": "finished",
                "response": response,
            })
        except Exception as e:
            return json.dumps({
                "type": "agent",
                "status": "error",
                "response": "An error occurred while generating the summary.",
                "error": str(e),
            })
    
    async def compress_user_query(self, question: str, name: str) -> dict:
        """
        Generate AI response for the summary as a single string.
        """
        # Initialize ChatGPT model
        summary_llm = ChatOpenAI(
            model=settings.OPENAI_CHAT_MODEL,
            temperature=0.7,
            streaming=False,
            openai_api_key=self.api_key
        )

        # Create a prompt
        prompt = PromptTemplate(template=QUERY_COMPRESS_PROMPT, input_variables=["question", "name"])

        # Create a LangChain LLMChain instance
        summarizing_chain = prompt | summary_llm | StrOutputParser()

        try:
            # Trigger the AI response
            response = await summarizing_chain.ainvoke({"question": question, "name": name})
            return json.dumps({
                "type": "agent",
                "status": "finished",
                "response": response,
            })
        except Exception as e:
            return json.dumps({
                "type": "agent",
                "status": "error",
                "response": "An error occurred while generating the summary.",
                "error": str(e),
            })
        
ai_chat_service = AIChatService()

